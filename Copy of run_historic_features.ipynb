{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5EWGko8Cm8I4","executionInfo":{"status":"ok","timestamp":1678404613374,"user_tz":0,"elapsed":28261,"user":{"displayName":"Alv Ja","userId":"15410870932638876894"}},"outputId":"e21139d5-7f02-42e4-a21d-484184e1e3c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install pip install tweet-preprocessor\n","!pip install pycountry\n","!pip install pandas --upgrade\n","!pip install transformers\n","!pip install torch\n","!pip install pysentimiento\n","!pip install wget\n","!pip install -Uqq ipdb\n","!pip install scipy --upgrade\n","import ipdb\n","%pdb off\n","!pip install tensorflow --upgrade\n","\n","!pip install language_tool_python"],"metadata":{"id":"-pULiT-TqHnk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import pandas as pd\n","import nltk\n","import nltk.data\n","nltk.download('vader_lexicon')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import torch\n","\n","import preprocessor as p\n","\n","import pycountry\n","import re\n","import string\n","from nltk import tokenize\n","from langdetect import detect\n","from nltk.stem import SnowballStemmer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import tensorflow as tf\n","import numpy as np\n","import random\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import os\n","\n","\n","\n","### topic modelling\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","import numpy as np\n","from scipy.special import expit\n","\n","#hate\n","from pysentimiento import create_analyzer\n","from pysentimiento.preprocessing import preprocess_tweet\n","\n","nltk.download('omw-1.4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WeVmHml0qJCK","executionInfo":{"status":"ok","timestamp":1676237503116,"user_tz":0,"elapsed":8137,"user":{"displayName":"Alvina","userId":"13758288938898952425"}},"outputId":"66d2165d-9ac3-4d72-c725-e5b3fef80807"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["device = torch.device(\"cuda\")"],"metadata":{"id":"13u5kk94sVhK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_topic_model(user_feeds):\n","        global topic_classes\n","\n","        MODEL = f\"cardiffnlp/tweet-topic-21-multi\"\n","\n","        with torch.no_grad():\n","            topic_model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n","\n","        topic_classes = topic_model.config.id2label#\n","\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","        tokens = user_feeds.clean_text.apply(lambda row: tokenizer(row, return_tensors='pt'))\n","\n","        print('loaded topic model')\n","        print(tokens)\n","        return topic_model , tokens"],"metadata":{"id":"gvSikE8f3Dg-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_psysentimento_model(user_feeds):\n","        \n","        tweets = user_feeds.clean_text.to_list()\n","\n","        # hateful\n","        analyzer = create_analyzer(task=\"hate_speech\", lang=\"en\")\n","        hate_labels = ['hateful', 'targeted', 'aggressive']\n","\n","        hate_out = [ analyzer.predict(preprocess_tweet(txt)) for txt in tweets ]\n","        print('predicted hate of tweets')\n","\n","        \n","        print('loaded hate model')\n","\n","        #emotion\n","        e_analyzer = create_analyzer(task=\"emotion\", lang=\"en\")\n","        emo_labels = ['joy','sadness','others','anger','surprise','disgust','fear']     \n","\n","\n","        emo_out = [ e_analyzer.predict(preprocess_tweet(txt)) for txt in tweets ]\n","        print('predicted emotion of tweets')\n","\n","        print('loaded emotion model')\n","\n","        return hate_out, emo_out"],"metadata":{"id":"Rj2v_ZhH3FL8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv"],"metadata":{"id":"DECikFwD_sz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tweet_cleaner(tw_list):\n","        print(\"yes\")\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",str(x))\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",str(x))\n","        hash = lambda x: re.sub(r'#', \"\", str(x))\n","        amp = lambda x: re.sub(r'&amp', \"\", str(x))\n","\n","\n","        print(tw_list[:10])\n","        tw_list['grammartext'] = tw_list['text'].map(remove_rt).map(rt)\n","        tw_list['clean_text'] = tw_list.text.map(remove_rt).map(rt)\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.HASHTAG)\n","        tw_list[\"grammartext\"] = tw_list.grammartext.map(p.clean)\n","\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        return tw_list"],"metadata":{"id":"mWJz-SiM3Gni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_sentiment(row):\n","        score = SentimentIntensityAnalyzer().polarity_scores(row)\n","        neg = score['neg']\n","        neu = score['neu']\n","        pos = score['pos']\n","        comp = score['compound']\n","        if neg > pos:\n","            \n","            label = \"negative\"\n","        elif pos > neg:\n","            \n","            label = \"positive\"\n","        else:\n","            \n","            label = \"neutral\"\n","        return [label, neg,neu,pos, comp]\n","\n","\n","\n","def get_topic(topic_model, row):\n","        output = topic_model(**row.to(device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = expit(scores)\n","        pred = np.argmax(scores)\n","        return [pred] + scores.flatten().tolist()\n","\n","\n","def get_hate(row):\n","        hate_labels = ['hateful', 'targeted', 'aggressive']\n","        return [row.probas[hate_labels[i]] for i in range(3) ]\n","\n","def get_emo(row):\n","        emo_labels = ['joy','sadness','others','anger','surprise','disgust','fear'] \n","        return [row.probas[emo_labels[i]] for i in range(7) ]\n"],"metadata":{"id":"uAQV6YMf3LGt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/final_year_project/historic_tweets/get_useful/history_tweets_retweeter_FINAL_FINAL_v3.csv')\n","\n","user_feeds = tweet_cleaner(df)\n","topic_model, topic_in = load_topic_model(user_feeds)\n","hate, emo = load_psysentimento_model(user_feeds)\n","\n","\n","sent = user_feeds.copy().clean_text.to_list()\n","grammer_in = user_feeds.grammartext.tolist()\n","\n","sent = user_feeds.copy().clean_text.to_list()\n","grammer_in = user_feeds.grammartext.tolist()\n","\n","\n","sent_output = [ get_sentiment(row) for row in sent]\n","\n","\n","topic_output = [ get_topic(topic_model, row) for row in topic_in]\n","\n","\n","hate_output = [ get_hate(row) for row in hate]\n","\n","\n","hate_output = abs(np.array(hate_output) - np.mean(hate_output, axis=0)).tolist() \n","hate_output = abs(np.array(hate_output) - np.mean(hate_output, axis=0)).tolist()\n","\n","emo_output = [ get_emo(row) for row in emo]\n","\n","\n","all = np.hstack((sent_output,topic_output,hate_output,emo_output))\n","\n","topics = [ val for _,val in topic_classes.items()]\n","\n","hate_labels = ['hateful', 'targeted', 'aggressive']\n","emo_labels = ['joy','sadness','others','anger','surprise','disgust','fear']     \n","\n","cols = ['sentiment','neg','neu','pos','comp','topic'] + topics + hate_labels + emo_labels\n","\n","\n","\n","user_feeds_df = pd.DataFrame(all.tolist())\n","user_feeds_df.columns = cols\n","\n","\n","user_feeds_df['id'] = list(user_feeds.index.values)\n","user_feeds_df['id'] = list(user_feeds['id'].tolist())\n","\n","int_cols = ['neg','neu','pos','comp'] + topics + hate_labels + emo_labels\n","\n","user_feeds_df[int_cols] = user_feeds_df.copy()[int_cols].astype(float)\n","\n","user_feeds = user_feeds_df.copy()\n","\n","del user_feeds_df # delete the old user_feeds_df\n","\n","fid = list(set(user_feeds['id'].tolist()))\n","\n","df = df.copy()[df['id'].isin(fid)]\n","\n","user_ids = df['id'].tolist()\n","\n","hates = ['hateful', 'targeted', 'aggressive']\n","emos = ['joy','sadness','others','anger','surprise','disgust','fear']     \n","\n","del topic_classes, hate_labels, emo_labels\n","\n","score_cols = ['id','neg','neu','pos','comp']+topics + hates + emos \n","\n","score_df = user_feeds[score_cols]\n","\n","label_df = user_feeds[['id','sentiment','topic']]\n","\n","def Average(lst):\n","    return sum(lst) / len(lst)\n","    \n","import random\n","from collections import Counter\n","from itertools import groupby\n","def cust_mode(l):\n","    freqs = groupby(Counter(l).most_common(), lambda x:x[1])\n","    return [val for val,count in next(freqs)[1]]\n","\n","all_s = []\n","all_mode = []\n","all_count = []\n","\n","\n","for user in user_ids:\n","\n","    s_df = score_df[score_df['id']==user].drop('id',axis=1).values.tolist()\n","    \n","    l_df = label_df[label_df['id']==user].drop('id',axis=1).values.tolist()\n","\n","    all_s.append([ Average(x) for x in zip(*s_df) ])\n","\n","    all_mode.append( [ random.choice(cust_mode(x)) for x in zip(*l_df)] )\n","\n","    counts = [ x for x in zip(*l_df) ] \n","\n","    sent_count = [ counts[0].count(s) for s in ['negative','neutral','positive'] ]\n","    topic_count = [ counts[1].count(str(float(s))) for s in range(19) ]\n","\n","    all_count.append(sent_count + topic_count)\n","\n","\n","new_cols = [ f'user_{x}_mean' for x in ['neg','neu','pos','comp']+topics + hates + emos ]# + ['politeness'] ]\n","\n","df[new_cols]  = all_s\n","\n","new_col = [ f'user_{x}_mode' for x in ['sentiment','topic']]\n","\n","df[new_col] = all_mode\n","\n","df.reset_index(drop=False)\n","df.set_index('id', inplace = True)\n","\n","\n","df.to_csv('/content/drive/MyDrive/Colab Notebooks/final_year_project/run_historic_features/1/historic_retweeter_features_msc_FINAL_NEW_v3.csv')"],"metadata":{"id":"iLiO5griy7v7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"04YNJRYKcGBd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","if 1435553820382269441 in df.id.values:\n","    print(\"yes\")"],"metadata":{"id":"DXtZ5oz_kp7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv('/content/drive/MyDrive/Colab Notebooks/final_year_project/run_historic_features/1/historic_retweeter_features_msc_FINAL_NEW_v3.csv')"],"metadata":{"id":"270bxMvpcMjz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"i_uduPpvcM_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GPRP8c-XcPFk"},"execution_count":null,"outputs":[]}]}