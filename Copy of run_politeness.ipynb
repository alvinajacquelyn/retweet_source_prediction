{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["The politeness model adapted from Berkem (past MSc student working on related work It is a pre-trained transformer model. I use the model he had pre-trained."],"metadata":{"id":"Jp5aJYwls7s0"}},{"cell_type":"code","source":["! pip install pandas==1.3.0\n","!pip install transformers\n","!pip install torch"],"metadata":{"id":"c64mECAnOBoG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EP8lALnwOE6S","executionInfo":{"status":"ok","timestamp":1678405160440,"user_tz":0,"elapsed":21346,"user":{"displayName":"Alv Ja","userId":"15410870932638876894"}},"outputId":"53e641e7-da35-4d6f-b69b-f094cd79472a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Train regresh"],"metadata":{"id":"fqyqfFIzScga"}},{"cell_type":"markdown","source":["# Run script"],"metadata":{"id":"YlTlxAAaSeuK"}},{"cell_type":"code","source":["# polite script\n","\n","from transformers.file_utils import is_tf_available, is_torch_available\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","from transformers import AutoConfig\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","\n","class SimpleDataset:\n","    def __init__(self, tokenized_texts):\n","        self.tokenized_texts = tokenized_texts\n","    \n","    def __len__(self):\n","        return len(self.tokenized_texts[\"input_ids\"])\n","    \n","    def __getitem__(self, idx):\n","        return {k: v[idx] for k, v in self.tokenized_texts.items()}\n","\n","\n","def compute_metrics_for_regression(eval_pred):\n","      logits, labels = eval_pred\n","\n","      labels = labels.reshape(-1, 1)\n","      \n","      print(\"Logits:\", logits[0:5])\n","      print(\"Labels:\", labels[0:5])\n","\n","\n","      mse = mean_squared_error(labels, logits)\n","      rmse = mean_squared_error(labels, logits, squared=False)\n","      mae = mean_absolute_error(labels, logits)\n","      r2 = r2_score(labels, logits)\n","\n","      single_squared_errors = ((logits - labels).flatten()**2).tolist()\n","      accuracy = sum([1 for e in single_squared_errors if e < 0.25]) / len(single_squared_errors)\n","      \n","      return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"accuracy\": accuracy}"],"metadata":{"id":"FEHi3Ny7LvL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","\n","def load_politeness_model(user_feeds):\n","    device = torch.device(\"cuda\")\n","    ## Scoring each tweet based on politeness \n","    class SimpleDataset:\n","        def __init__(self, tokenized_texts):\n","            self.tokenized_texts = tokenized_texts\n","        \n","        def __len__(self):\n","            return len(self.tokenized_texts[\"input_ids\"])\n","        \n","        def __getitem__(self, idx):\n","            return {k: v[idx] for k, v in self.tokenized_texts.items()}\n","\n","    model_name = 'roberta-base'\n","    print('loaded politeness')\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","    with torch.no_grad():\n","        model = AutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/Colab Notebooks/final_year_project/checkpoint-52500/').to(device)\n","\n","    tweets = df[\"grammartext\"].astype(str).values.tolist()\n","\n","    test_encodings = tokenizer(tweets , truncation=True, padding=True, max_length=256)\n","    test_dataset = SimpleDataset(test_encodings)       \n","\n","    trainer = Trainer(model=model)\n","    predictions = trainer.predict(test_dataset)\n","    \n","    return predictions[0]\n","\n"],"metadata":{"id":"_ehX71QZL6ls"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/final_year_project/run_historic_features/1/historic_retweeter_features_msc_FINAL_NEW_v3.csv')\n","\n","import torch\n","polite_out = load_politeness_model(df)\n","df['politeness'] = polite_out\n","\n","save_path = '/content/drive/MyDrive/Colab Notebooks/final_year_project/run_historic_features/2/historic_retweeter_features_msc_FINAL_NEW_withpoliteness_v3.csv'\n","\n","df.to_csv(save_path)"],"metadata":{"id":"VGOHCGLL7y_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"qNvsOn393dtc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if 1435553820382269441 in df.id.values:\n","    print(\"yes\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Umw8Pr4EW2H","executionInfo":{"status":"ok","timestamp":1676247348751,"user_tz":0,"elapsed":5,"user":{"displayName":"Alvina","userId":"13758288938898952425"}},"outputId":"56f78f87-956b-4353-867d-ea3c79188a8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["yes\n"]}]}]}