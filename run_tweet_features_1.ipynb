{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bolcXHfprOWd","executionInfo":{"status":"ok","timestamp":1678401226324,"user_tz":0,"elapsed":2794,"user":{"displayName":"Alv Ja","userId":"15410870932638876894"}},"outputId":"bb648023-1df1-4bae-b9f7-f7d11a8671b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import itertools"],"metadata":{"id":"c3kwV-6Bsvvr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pandas==1.3.0"],"metadata":{"id":"w1VRDFr4eSGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pip install tweet-preprocessor\n","!pip install pycountry\n","# !pip install pandas --upgrade\n","!pip install transformers\n","!pip install xgboost\n","!pip install torch\n","!pip install mislib\n","!pip install pysentimiento\n","!pip install wget\n","!pip install -Uqq ipdb\n","!pip install scipy --upgrade\n","import ipdb\n","%pdb off\n","!pip install tensorflow --upgrade\n"],"metadata":{"id":"yB3R_CjkszL1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","nltk.download('vader_lexicon')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import json\n","import pickle\n","import joblib\n","import torch\n","import preprocessor as p\n","import pycountry\n","import re\n","import string\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from nltk import tokenize\n","from sklearn.feature_extraction.text import CountVectorizer\n","import tensorflow as tf\n","import random\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from datasets import Dataset\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","from scipy.special import expit\n","\n","from pysentimiento import create_analyzer\n","from pysentimiento.preprocessing import preprocess_tweet\n","\n","nltk.download('omw-1.4')"],"metadata":{"id":"yF7t_T57tsDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\")"],"metadata":{"id":"_efvnNPy1EVb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_topic_model(user_feeds):\n","        global topic_classes\n","\n","        MODEL = f\"cardiffnlp/tweet-topic-21-multi\"\n","\n","        with torch.no_grad():\n","            topic_model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n","\n","        topic_classes = topic_model.config.id2label#\n","\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","        tokens = user_feeds.clean_text.apply(lambda row: tokenizer(row, return_tensors='pt'))\n","\n","        print('loaded topic model')\n","        print(tokens)\n","        return topic_model , tokens"],"metadata":{"id":"-0-vcE280ho-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_psysentimento_model(user_feeds):\n","        \n","        tweets = user_feeds.clean_text.to_list()\n","        analyzer = create_analyzer(task=\"hate_speech\", lang=\"en\")\n","        hate_labels = ['hateful', 'targeted', 'aggressive']\n","\n","        hate_out = [ analyzer.predict(preprocess_tweet(txt)) for txt in tweets ]\n","\n","        e_analyzer = create_analyzer(task=\"emotion\", lang=\"en\")\n","        emo_labels = ['joy','sadness','others','anger','surprise','disgust','fear']     \n","\n","        emo_out = [ e_analyzer.predict(preprocess_tweet(txt)) for txt in tweets ]\n","\n","        return hate_out, emo_out"],"metadata":{"id":"zJB6AGfd1Sep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tweet_cleaner(tw_list):\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",str(x))\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",str(x))\n","        hash = lambda x: re.sub(r'#', \"\", str(x))\n","        amp = lambda x: re.sub(r'&amp', \"\", str(x))\n","\n","        print(\"tweet_cleaner\")\n","        print(tw_list[:10])\n","        tw_list['grammartext'] = tw_list['text'].map(remove_rt).map(rt)\n","        print(\"grammar\")\n","        tw_list['clean_text'] = tw_list.text.map(remove_rt).map(rt)\n","        print(\"clean_text\")\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.HASHTAG)\n","        tw_list[\"grammartext\"] = tw_list.grammartext.map(p.clean)\n","        print(\"grammar\")\n","\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.map(p.clean).map(hash).map(amp)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.str.lower()\n","        return tw_list"],"metadata":{"id":"yq_BBteWxblG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_sentiment(row):\n","        score = SentimentIntensityAnalyzer().polarity_scores(row)\n","        neg = score['neg']\n","        neu = score['neu']\n","        pos = score['pos']\n","        comp = score['compound']\n","        if pos< neg:       \n","            label = \"negative\"\n","        elif pos > neg:       \n","            label = \"positive\"\n","        else:      \n","            label = \"neutral\"\n","        return [label, neg,neu,pos, comp]\n","\n","\n","\n","def get_topic(topic_model, row):\n","        # tokens = self.topic_tokenizer(row.clean_text,return_tensors='pt')\n","        output = topic_model(**row.to(device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = expit(scores)\n","        pred = np.argmax(scores)\n","        return [pred] + scores.flatten().tolist()\n","\n","\n","def get_hate(row):\n","        hate_labels = ['hateful', 'targeted', 'aggressive']\n","        return [row.probas[hate_labels[i]] for i in range(3) ]\n","\n","def get_emo(row):\n","        emo_labels = ['joy','sadness','others','anger','surprise','disgust','fear'] \n","        return [row.probas[emo_labels[i]] for i in range(7) ]\n","\n","\n","def get_readability(row):\n","        if not row:\n","            return [0]*23\n","        else:\n","            results = readability.getmeasures(row,lang='en')\n","            # [ df.loc[index, score] = results['readability grades'][score] for score in \\\n","            #  ['Kincaid','ARI', 'Coleman-Liau', 'FleschReadingEase', 'GunningFogIndex', \\\n","            #   'LIX', 'SMOGIndex', 'RIX', 'DaleChallIndex'] ]\n","\n","            return [ grade[t] for grade in [results['readability grades'], results['sentence info'] ] for t in grade ]\n"],"metadata":{"id":"WQUh3_HO2G7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","hashtags = ['#VoteThemOut',\n","            # '#ToryScumOut',\n","            # 'F1',\n","            'Lisa',\n","            '#EnoughIsEnough',\n","            '#EnergyPrices',\n","            '#iOS16',\n","            '#taiwan',\n","            '#onepiece',\n","            '#CostOfLivingCrisis',\n","            '#GetBackToWorkYouFatPonce', \n","            # '#ClosingCeremony',\n","            '#BookLoversDay',\n","            '#biden']\n","# add back            \n","# hashtags = ['F1']\n","\n","for hash in hashtags:\n","        df = pd.read_csv(f'/content/drive/MyDrive/Colab Notebooks/final_year_project/raw_data/{hash}.csv') \n","        #id,\ttext,\tcreated_at,\tuser_id,\tfollowers_count,\tfriends_count\t,favourites_count,\tretweet_count\t\n","        print(df[df['user_id'].isnull()])\n","        df['user_id'] = pd.to_numeric(df['user_id'], errors='coerce')\n","        df = df.dropna(subset=['user_id'])\n","        df['user_id'] = df['user_id'].astype(int)\n","        df['user_id']= df['user_id'].map(str)\n","        \n","        user_feeds = tweet_cleaner(df)\n","        topic_model, topic_in = load_topic_model(user_feeds)\n","        hate, emo = load_psysentimento_model(user_feeds)\n","\n","        read_cols = ['Kincaid', 'ARI', 'Coleman-Liau', 'FleschReadingEase', 'GunningFogIndex', \n","                    'LIX', 'SMOGIndex', 'RIX', 'DaleChallIndex','characters_per_word', 'syll_per_word', \n","                    'words_per_sentence', 'sentences_per_paragraph', 'type_token_ratio', 'characters', 'syllables', \n","                    'words', 'wordtypes', 'sentences', 'paragraphs', 'long_words', 'complex_words', 'complex_words_dc']    \n","\n","\n","\n","        sent = user_feeds.copy().clean_text.to_list()\n","        grammer_in = user_feeds.grammartext.tolist()\n","        read_in = user_feeds.clean_text.tolist()\n","\n","\n","        sent = user_feeds.copy().clean_text.to_list()\n","        grammer_in = user_feeds.grammartext.tolist()\n","        read_in = user_feeds.clean_text.tolist()\n","        \n","        sent_output = [ get_sentiment(row) for row in sent]\n","        print('got sentiment')\n","\n","        topic_output = [ get_topic(topic_model, row) for row in topic_in]\n","        print('got topic')\n","\n","        hate_output = [ get_hate(row) for row in hate]\n","        print('got hate ')\n","\n","        hate_output = abs(np.array(hate_output) - np.mean(hate_output, axis=0)).tolist() # fix error in hate classification\n","        hate_output = abs(np.array(hate_output) - np.mean(hate_output, axis=0)).tolist()\n","\n","        emo_output = [ get_emo(row) for row in emo]\n","        print('got emo')\n","\n","        read_out = [ get_readability(row) for row in read_in]\n","        print('got readability scores ')\n","\n","\n","        \n","        # PLACING ALL INTO THE USER FEEDS DF!!!!!!\n","        all = np.hstack((sent_output,topic_output,hate_output,emo_output, read_out))\n","\n","        topics = [ val for _,val in topic_classes.items()]\n","\n","        hate_labels = ['hateful', 'targeted', 'aggressive']\n","        emo_labels = ['joy','sadness','others','anger','surprise','disgust','fear']     \n","\n","        cols = ['sentiment','neg','neu','pos','comp','topic'] + topics + hate_labels + emo_labels + read_cols\n","\n","        user_feeds_df = pd.DataFrame(all.tolist())\n","        user_feeds_df.columns = cols\n","\n","\n","        user_feeds_df['id'] = list(user_feeds.index.values)\n","        user_feeds_df['user_id'] = list(user_feeds['user_id'].tolist())\n","\n","\n","        int_cols = ['neg','neu','pos','comp'] + topics + hate_labels + emo_labels + read_cols \n","\n","        user_feeds_df[int_cols] = user_feeds_df.copy()[int_cols].astype(float)\n","\n","        user_feeds = user_feeds_df.copy()\n","\n","        del user_feeds_df\n","\n","        fid = list(set(user_feeds['user_id'].tolist()))\n","\n","        df = df.copy()[df['user_id'].isin(fid)]\n","\n","        user_ids = df['user_id'].tolist()\n","\n","        hates = ['hateful', 'targeted', 'aggressive']\n","        emos = ['joy','sadness','others','anger','surprise','disgust','fear']     \n","\n","        del topic_classes, hate_labels, emo_labels        \n","\n","        score_cols = ['user_id','neg','neu','pos','comp']+topics + hates + emos + read_cols\n","\n","        score_df = user_feeds[score_cols]\n","\n","        label_df = user_feeds[['user_id','sentiment','topic']]\n","\n","        def Average(lst):\n","            return sum(lst) / len(lst)\n","          \n","        import random\n","        from collections import Counter\n","        from itertools import groupby\n","        def cust_mode(l):\n","            freqs = groupby(Counter(l).most_common(), lambda x:x[1])\n","            return [val for val,count in next(freqs)[1]]\n","\n","        all_s = []\n","        all_mode = []\n","        all_count = []\n","\n","        for user in user_ids:\n","\n","            s_df = score_df[score_df['user_id']==user].drop('user_id',axis=1).values.tolist()\n","            \n","            l_df = label_df[label_df['user_id']==user].drop('user_id',axis=1).values.tolist()\n","\n","            all_s.append([ Average(x) for x in zip(*s_df) ])\n","\n","            all_mode.append( [ random.choice(cust_mode(x)) for x in zip(*l_df)] )\n","\n","            counts = [ x for x in zip(*l_df) ] \n","\n","            sent_count = [ counts[0].count(s) for s in ['negative','neutral','positive'] ]\n","            topic_count = [ counts[1].count(str(float(s))) for s in range(19) ]\n","\n","            all_count.append(sent_count + topic_count)\n","\n","\n","        new_cols = [ f'user_{x}_mean' for x in ['neg','neu','pos','comp']+topics + hates + emos + read_cols ]# + ['politeness'] ]\n","\n","        df[new_cols]  = all_s\n","\n","        new_col = [ f'user_{x}_mode' for x in ['sentiment','topic']]\n","\n","        df[new_col] = all_mode\n","\n","        df.reset_index(drop=False)\n","        df.set_index('id', inplace = True)\n","\n","        df.to_csv(f'/content/drive/MyDrive/Colab Notebooks/final_year_project/run_tweet_features/1/{hash}_tweet_features.csv')"],"metadata":{"id":"caHizxb9umPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['user_id'][22]"],"metadata":{"id":"NhAJaJzxyyEY"},"execution_count":null,"outputs":[]}]}