{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["The offensive model adapted from Berkem (past MSc student working on related work)."],"metadata":{"id":"XwiSw2nFtuw5"}},{"cell_type":"code","source":["!pip install datasets\n","!pip install pycountry\n","!pip install pandas==1.2.3\n","!pip install transformers\n","!pip install torch"],"metadata":{"id":"BzyV-H5qno5u"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g92GVTdKjrDS"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import transformers\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","\n","#irony\n","import urllib.request\n","from scipy.special import softmax\n","import csv\n","\n","# offensiveness\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\n"]},{"cell_type":"code","source":["import torch"],"metadata":{"id":"fIqyzUdKoOxZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TFQ_UWDelt9T","executionInfo":{"status":"ok","timestamp":1673811697642,"user_tz":0,"elapsed":29997,"user":{"displayName":"Alvina","userId":"13758288938898952425"}},"outputId":"c28f6203-969e-4aaf-d4b4-d11a77206fdb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks/Ziwen \n"]}]},{"cell_type":"code","source":["def run_offensive_model(test):\n","    df_scraped = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/final_year_project/run_tweet_features/3/offensiveness/labeled_tweets.csv')\n","    df_public = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/final_year_project/run_tweet_features/3/offensiveness/public_data_labeled.csv')\n","    df_scraped.drop_duplicates(inplace = True)\n","    df_scraped.drop('id', axis = 'columns', inplace = True)\n","    df_public.drop_duplicates(inplace = True)\n","    df = pd.concat([df_scraped, df_public])\n","    df['label'] = df.label.map({'Offensive': 1, 'Non-offensive': 0})\n","    X_train, X_test, y_train, y_test = train_test_split(df['full_text'], \n","                                                df['label'], \n","                                                random_state=42)\n","\n","\n","    count_vector = CountVectorizer(stop_words = 'english', lowercase = True)\n","\n","    training_data = count_vector.fit_transform(X_train)\n","    testing_data = count_vector.transform(test)\n","    model = SGDClassifier()\n","    model.fit(training_data, y_train)\n","    preds = model.predict(testing_data)\n","    return preds\n","\n","\n","def load_offensive_model(df):\n","    test_data = df.text\n","    preds = run_offensive_model(test_data)\n","    df['offensive'] = preds\n","    return df\n","\n","\n","def load_irony_model(df):\n","    device = torch.device(\"cuda\")\n","    task='irony'\n","    MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","    tokens = df.text.apply(lambda row: tokenizer(row, return_tensors='pt'))\n","    # download label mapping\n","    mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n","    with urllib.request.urlopen(mapping_link) as f:\n","        html = f.read().decode('utf-8').split(\"\\n\")\n","        csvreader = csv.reader(html, delimiter='\\t')\n","    irony_labels = [row[1] for row in csvreader if len(row) > 1]\n","    # PT\n","    with torch.no_grad():\n","        global irony_model \n","        irony_model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n","    df['cardiff_tokens'] = tokens\n","    print('loaded irony model for tweets')\n"],"metadata":{"id":"2w9uczCmk2Iz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_irony(df,index,row):\n","    output = irony_model(**row.cardiff_tokens.to(device))\n","    scores = output[0][0].detach().cpu().numpy()\n","    scores = softmax(scores)\n","\n","    ranking = np.argsort(scores)\n","    ranking = ranking[::-1]\n","\n","    df.loc[index, 'irony'] = ranking[0]\n","\n","    \n","\n","def get_offensive(df,index,row):\n","    df.loc[index, 'offensive'] = row.offensive"],"metadata":{"id":"QGo1N4XsqPGh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hashtags = ['#VoteThemOut',\n","            # '#ToryScumOut',\n","            # 'F1',\n","            'Lisa',\n","            '#EnoughIsEnough',\n","            '#EnergyPrices',\n","            '#iOS16',\n","            '#taiwan',\n","            '#onepiece',\n","            '#CostOfLivingCrisis',\n","            '#GetBackToWorkYouFatPonce', \n","            # '#ClosingCeremony',\n","            '#BookLoversDay',\n","            '#biden']\n","            \n","# add this \n","# hashtags = ['F1']\n","\n","for hash in hashtags:\n","        device = torch.device(\"cuda\")\n","        df = pd.read_csv(f'/content/drive/MyDrive/Colab Notebooks/final_year_project/run_tweet_features/2/{hash}_tweet_features_with_politeness.csv') \n","        #id,\ttext,\tcreated_at,\tuser_id,\tfollowers_count,\tfriends_count\t,favourites_count,\tretweet_count\n","\n","\n","        loaders = [load_irony_model, load_offensive_model]\n","\n","        [load(df) for load in loaders]\n","\n","        score_funcs = [get_offensive, get_irony]\n","\n","        [ func(df,index,row) for func in score_funcs for index, row in df.iterrows() ]\n","\n","        df.to_csv(f'/content/drive/MyDrive/Colab Notebooks/final_year_project/run_tweet_features/3/{hash}_tweet_features_with_offensiveirony.csv')\n"," "],"metadata":{"id":"NQavN_fcj-oA"},"execution_count":null,"outputs":[]}]}