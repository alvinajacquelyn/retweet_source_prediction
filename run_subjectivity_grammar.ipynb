{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"uHgb5rsuWD8H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import pandas as pd\n","import string\n","import spacy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","from statistics import mode"],"metadata":{"id":"Tr9Ouj7_WETs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install language-tool-python"],"metadata":{"id":"mia_KYlfXQIW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import language_tool_python\n","tool = language_tool_python.LanguageTool('en-US')"],"metadata":{"id":"zxFDJrKqXIZc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tweet-preprocessor\n","import preprocessor as p"],"metadata":{"id":"jvU_zWsOYyeT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tweet_cleaner(tw_list):\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",str(x))\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",str(x))\n","        hash = lambda x: re.sub(r'#', \"\", str(x))\n","        amp = lambda x: re.sub(r'&amp', \"\", str(x))\n","\n","        print(\"tweet_cleaner\")\n","        print(tw_list[:10])\n","        tw_list['grammartext'] = tw_list['text'].map(remove_rt).map(rt)\n","        print(\"grammar\")\n","        tw_list['clean_text'] = tw_list.text.map(remove_rt).map(rt)\n","        print(\"clean_text\")\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.HASHTAG)\n","        tw_list[\"grammartext\"] = tw_list.grammartext.map(p.clean)\n","        print(\"grammar\")\n","\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.map(p.clean).map(hash).map(amp)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.str.lower()\n","        # print(tw_list)\n","        return tw_list"],"metadata":{"id":"IkHK1YccXgYt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from textblob import TextBlob"],"metadata":{"id":"TaXt8BxQYr3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hashtags = ['#VoteThemOut',\n","            # '#ToryScumOut',\n","            'F1',\n","            'Lisa',\n","            '#EnoughIsEnough',\n","            '#EnergyPrices',\n","            '#iOS16',\n","            '#taiwan',\n","            '#onepiece',\n","            '#CostOfLivingCrisis',\n","            '#GetBackToWorkYouFatPonce', \n","            # '#ClosingCeremony',\n","            '#BookLoversDay',\n","            '#biden']\n","# hashtags = ['F1']\n","for hash in hashtags:\n","        grammarlist = []\n","        subjectivitylist = []\n","        df=pd.read_csv(f'/content/drive/MyDrive/Colab Notebooks/final_year_project/run_tweet_features/4/{hash}_tweet_features_with_gender.csv')\n","        user_feeds = tweet_cleaner(df)\n","        tweets = user_feeds.clean_text.to_list()\n","        print(len(tweets))\n","        for tweet in tweets:\n","            matches = int(len(tool.check(str(tweet))))\n","            grammarlist.append(matches)\n","            analysis = TextBlob(tweet)\n","            subjectivitylist.append(analysis.sentiment.subjectivity)\n","        df['tweet_grammar'] = grammarlist\n","        df['tweet_subjectivity'] = subjectivitylist\n","\n","        df.to_csv(f'/content/drive/MyDrive/Colab Notebooks/final_year_project/run_tweet_features/5/{hash}_tweet_features_with_subjectivitygrammar.csv')\n","        \n","\n","        \n","\n"],"metadata":{"id":"S42Sa--MWfDd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run subjectivity"],"metadata":{"id":"-qzaKlejZ9FG"}}]}